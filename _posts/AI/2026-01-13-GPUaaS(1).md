---
title: [AI스터디] GPUaaS(1)
date: 2026-01-13 11:11:11 
categories: [AI스터디, GPUaaS, cloud, AI infra]
tags: [DxPU, RDMA, CXL]     # TAG names should always be lowercase
---
# 이 주제를 선택한 이유
https://www.technologyreview.com/2026/01/12/1130697/10-breakthrough-technologies-2026/
에서 선정한 주제에 AI data center 라는 내용이 있고
회사에서 최근 GPUaaS 사업에 힘을 쏟고 있어서 
follow-up 하기 위해 선택

AI infrastructure에 대한 기초적인 내용 조사 

# gpu/tpu/npu
AI 계산을 빠르게 하려고 만든 전용 계산기
| 구분 |	GPU|	TPU|	NPU|
|-----|-----|-----|-----|
|주요 목적	|범용 병렬 연산|	AI 학습/추론	(딥러닝 연산 tensor 만 빠르게 처리) |AI 추론|
|주 사용처|	클라우드|	Google Cloud	|디바이스|
|학습	|⭕|	⭕|	❌|
|추론	|⭕	|⭕|	⭕|
|전력 효율|	❌|	⭕	⭐⭐⭐⭐⭐|
|범용성	|⭐⭐⭐⭐⭐|	⭐⭐|	⭐|
|대표	|NVIDIA	|Google|	Apple / Qualcomm|

# GPU disaggregation
I/O virtualization을 위해서 cpu/gpu를 같은 host server의 vm에 분리, 할당. 하지만 flexible gpu provisiong에 실패. 유휴상태의 host server 위의 1개 이상의 gpu에 대해서 다른 host server의 vm에서 이 gpu를 사용 불가. 이러한 한계점에 대해 gpu disaggregation이 제안됨.
--> host server로 부터 gpu를 physically decouple

### 다음 레벨에서 구현할 수 있다.
1. 사용자 공간 라이브러리(user-mode)
2. 커널 드라이버(kernel-mode)
3. PCIe 하드웨어 레벨

### library, driver 수준에서는 3가지 한계점이 있다. 
#### (1) 적용 범위가 제한적
특정 API / 특정 GPU 스택에 강하게 의존<br>
CUDA를 가로채서(disaggregate) 만든 시스템은 CUDA API를 쓰는 앱만 지원
(OpenGL / Vulkan / DirectX 앱은 지원 x)
#### (2) 지속적인 유지보수 필요
CUDA, GPU 드라이버는 계속 버전이 바뀜
#### (3) 성능 최적화가 어렵다
user / kernel 레벨에서는 GPU 명령을 가로채고 네트워크로 전달하고 다시 재구성해야 함. 이 과정에서 latency 증가, context switching 발생.

PCIe 레벨에서 접근하면 (1) full-scenario 지원 (2) software transparency (3) good performance in nature

## PCIe 레벨에서 구현한 GPU disaggregation
PCIe fabric 위에 구축된 솔루션은 scope와 capacity에 아쉬움이 있다. 

- PCIe fabric : 서버 ↔ 스위치 ↔ GPU를 PCIe 케이블로 직접 연결한 구조. 

이 구조는 몇 개 rack 정도까지만 확장 가능(데이터 센터 전체로 확장 할 수 없다) 또한 낮은 BER을 유지해야하므로 신호 품질이 엄격하고 스위치/연결 수에 제한 이 있다. 

https://developer.nvidia.com/blog/hgx-2-fuses-ai-computing/?utm_source=chatgpt.com
https://codywu2010.wordpress.com/2015/11/26/pci-express-max-read-request-max-payload-size-and-why-you-care/?utm_source=chatgpt.com
https://www.h3platform.com/blog-detail/13?utm_source=chatgpt.com

## 실제 PCIe-level 구현 예시
1. PCIe Fabric 기반
- 서버 ↔ PCIe 스위치 ↔ GPU
- 케이블로 직접 연결

문제:
- 거리 짧음
- 랙 단위 한계

2. PCIe over Network (RDMA)
-PCIe 트랜잭션을 네트워크 패킷으로 전달
- PCIe 프로토콜 의미 (MMIO, DMA, interrupt, ordering)만 보존
- 전송은 RDMA로 처리

기존 구조 : CPU ─ PCIe ─ GPU   (같은 서버)
PCIe-level disaggregation : CPU ─ PCIe semantics ─ Network / Fabric ─ Remote GPU

CPU가 GPU에 접근하면 “PCIe read/write” 요청 발생

이 요청을 가로채서 네트워크로 캡슐화(encapsulation)

원격 서버로 전달(RDMA / 전용 fabric / CXL 등)

원격 GPU가 실행 후 결과를 다시 PCIe 응답처럼 반환

3. CXL 기반
- PCIe 위에서 동작하는 cache/memory coherent 인터커넥트
- GPU/메모리를 풀(pool)로 구성 가능
- CXL Consoritium이 표준화 중, NVIDA/Intel/AMD 참여 중

LLM추론은 파라미터/KV cache/ 중간 activations 때문에 GPU 가 남아도 메모리 부족 발생.
--> AI data center는 전력과 물 소비량이 엄청난데 그 이유는 GPU, memory를 묶어 써서 낭비하기 때문

